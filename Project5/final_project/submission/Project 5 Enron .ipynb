{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Enron scandal of the early 2000's led to the eventual bankruptcy of a multi-billion dollar company. Enron misled shareholders and the public about their profitability and debt through a series of accounting tricks, deception, and outright fraud. The case was widely covered by news outlets, and also saw a lot of usually confidential documents make their way into the public domain: e-mails and financial data about employees. This provides a useful dataset for exploring machine learning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Goal and Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\") \n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data, test_classifier\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>bonus</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>...</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>other</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>poi</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <th>email_address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ALLEN PHILLIP K</th>\n",
       "      <td>201955.0</td>\n",
       "      <td>2902.0</td>\n",
       "      <td>2869717.0</td>\n",
       "      <td>4484442.0</td>\n",
       "      <td>1729541.0</td>\n",
       "      <td>4175000.0</td>\n",
       "      <td>126027.0</td>\n",
       "      <td>1407.0</td>\n",
       "      <td>-126027.0</td>\n",
       "      <td>1729541.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2195.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3081055.0</td>\n",
       "      <td>304805.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>phillip.allen@enron.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BADUM JAMES P</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>178980.0</td>\n",
       "      <td>182466.0</td>\n",
       "      <td>257817.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>257817.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BANNANTINE JAMES M</th>\n",
       "      <td>477.0</td>\n",
       "      <td>566.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>916197.0</td>\n",
       "      <td>4046157.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1757552.0</td>\n",
       "      <td>465.0</td>\n",
       "      <td>-560222.0</td>\n",
       "      <td>5243487.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.0</td>\n",
       "      <td>864523.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5104.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.0</td>\n",
       "      <td>james.bannantine@enron.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAXTER JOHN C</th>\n",
       "      <td>267102.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1295738.0</td>\n",
       "      <td>5634343.0</td>\n",
       "      <td>6680544.0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>3942714.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10623258.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2660303.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1386055.0</td>\n",
       "      <td>1586055.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAY FRANKLIN R</th>\n",
       "      <td>239671.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>260455.0</td>\n",
       "      <td>827696.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>145796.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-82782.0</td>\n",
       "      <td>63014.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-201641.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>frank.bay@enron.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      salary  to_messages  deferral_payments  total_payments  \\\n",
       "ALLEN PHILLIP K     201955.0       2902.0          2869717.0       4484442.0   \n",
       "BADUM JAMES P            NaN          NaN           178980.0        182466.0   \n",
       "BANNANTINE JAMES M     477.0        566.0                NaN        916197.0   \n",
       "BAXTER JOHN C       267102.0          NaN          1295738.0       5634343.0   \n",
       "BAY FRANKLIN R      239671.0          NaN           260455.0        827696.0   \n",
       "\n",
       "                    exercised_stock_options      bonus  restricted_stock  \\\n",
       "ALLEN PHILLIP K                   1729541.0  4175000.0          126027.0   \n",
       "BADUM JAMES P                      257817.0        NaN               NaN   \n",
       "BANNANTINE JAMES M                4046157.0        NaN         1757552.0   \n",
       "BAXTER JOHN C                     6680544.0  1200000.0         3942714.0   \n",
       "BAY FRANKLIN R                          NaN   400000.0          145796.0   \n",
       "\n",
       "                    shared_receipt_with_poi  restricted_stock_deferred  \\\n",
       "ALLEN PHILLIP K                      1407.0                  -126027.0   \n",
       "BADUM JAMES P                           NaN                        NaN   \n",
       "BANNANTINE JAMES M                    465.0                  -560222.0   \n",
       "BAXTER JOHN C                           NaN                        NaN   \n",
       "BAY FRANKLIN R                          NaN                   -82782.0   \n",
       "\n",
       "                    total_stock_value             ...              \\\n",
       "ALLEN PHILLIP K             1729541.0             ...               \n",
       "BADUM JAMES P                257817.0             ...               \n",
       "BANNANTINE JAMES M          5243487.0             ...               \n",
       "BAXTER JOHN C              10623258.0             ...               \n",
       "BAY FRANKLIN R                63014.0             ...               \n",
       "\n",
       "                    loan_advances  from_messages      other  \\\n",
       "ALLEN PHILLIP K               NaN         2195.0      152.0   \n",
       "BADUM JAMES P                 NaN            NaN        NaN   \n",
       "BANNANTINE JAMES M            NaN           29.0   864523.0   \n",
       "BAXTER JOHN C                 NaN            NaN  2660303.0   \n",
       "BAY FRANKLIN R                NaN            NaN       69.0   \n",
       "\n",
       "                    from_this_person_to_poi    poi director_fees  \\\n",
       "ALLEN PHILLIP K                        65.0  False           NaN   \n",
       "BADUM JAMES P                           NaN  False           NaN   \n",
       "BANNANTINE JAMES M                      0.0  False           NaN   \n",
       "BAXTER JOHN C                           NaN  False           NaN   \n",
       "BAY FRANKLIN R                          NaN  False           NaN   \n",
       "\n",
       "                    deferred_income  long_term_incentive  \\\n",
       "ALLEN PHILLIP K          -3081055.0             304805.0   \n",
       "BADUM JAMES P                   NaN                  NaN   \n",
       "BANNANTINE JAMES M          -5104.0                  NaN   \n",
       "BAXTER JOHN C            -1386055.0            1586055.0   \n",
       "BAY FRANKLIN R            -201641.0                  NaN   \n",
       "\n",
       "                    from_poi_to_this_person               email_address  \n",
       "ALLEN PHILLIP K                        47.0     phillip.allen@enron.com  \n",
       "BADUM JAMES P                           NaN                         NaN  \n",
       "BANNANTINE JAMES M                     39.0  james.bannantine@enron.com  \n",
       "BAXTER JOHN C                           NaN                         NaN  \n",
       "BAY FRANKLIN R                          NaN         frank.bay@enron.com  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "df = pd.concat([df.drop('email_address', axis=1).apply(pd.to_numeric, errors='coerce'), \\\n",
    "                df['email_address']], axis=1) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to develop a classifier which can help identify persons of interest in the Enron scandal. A person of interest is someone who was indicted, who settled without admitting guilt, or who testified in exchange for immunity. The dataset we have has information about the finances of employees, including things like salary, stock, and bonuses and about the e-mail practices of the employees, including things like how many time they sent messages to POIs. The dataset contains 146 rows, 18 of which are identified as POIs. There are 21 features in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for outliers, my main concern with the data was whether there were rows that would significantly skew classification. The salary information and e-mail information is presumably correct unless there was error in entering it; it is a fact of the features that there are significantly larger dollar values for some records, like Kenneth Lay's. This nature of the data could perhaps be mitigated with logarithmic scaling, but was not needed to achieve the model accuracy necessary. \n",
    "\n",
    "There were some rows that were odd and unhelpful for training or testing. Most obviously, the 'TOTAL' and 'THE TRAVEL AGENCY IN THE PARK' rows were unhelpful for finding POIs. However, some non-POIs in the dict were determined to also be non-helpful due to the large amount of null values they had in useful features. These included the rows for names 'LOCKHART EUGENE E', 'WODRASKA JOHN','SCRIMSHAW MATTHEW','WHALEY DAVID A','GRAMM WENDY L', and 'WROBEL BRUCE'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.drop(['TOTAL','THE TRAVEL AGENCY IN THE PARK'])    \n",
    "df = df.drop(['LOCKHART EUGENE E','WODRASKA JOHN','SCRIMSHAW MATTHEW','WHALEY DAVID A','GRAMM WENDY L','WROBEL BRUCE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xd09cef0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEFCAYAAAAmIwo/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb8a80f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot('poi', 'salary',\n",
    "           data=df[df['salary'] < df['salary'].quantile(.95)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0xb8a8400>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEPCAYAAABLIROyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+NhuOQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xce794a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot('poi', 'total_stock_value',\n",
    "           data=df[df['total_stock_value'] < df['total_stock_value'].quantile(.95)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “properly scale features”, “intelligently select feature”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The features that ended up in my model were 'exercised_stock_options', 'total_stock_value', 'bonus'. These were ultimately chosen by a loop which averaged the results of SelectKBest over many folds of StratifiedShuffleSplit after paring down the feature list according to whether there was enough useful data; those columns that had a lot of missing values were eliminated. An extreme case was 'loan_advances' which had data for only 3 people but 'director_fees', 'restricted_stock_deferred', 'deferral_payments', and 'deferred_income' were dropped for the same reason. Additionally, 'other' and 'email_address' were dropped, the former for being too non-descriptive and the latter because it wouldn't work with my choice in classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drop_columns = ['other', 'email_address', 'director_fees', 'loan_advances', \n",
    "                'restricted_stock_deferred', 'deferral_payments', 'deferred_income']\n",
    "df = df.drop(drop_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "exercised_stock_options    23.406684\n",
       "total_stock_value          20.073231\n",
       "bonus                      13.673070\n",
       "salary                      9.442976\n",
       "long_term_incentive         7.519849\n",
       "restricted_stock            7.302580\n",
       "total_payments              6.996585\n",
       "shared_receipt_with_poi     6.286249\n",
       "from_poi_to_this_person     3.678794\n",
       "from_this_person_to_poi     2.025036\n",
       "expenses                    0.929172\n",
       "to_messages                 0.791943\n",
       "from_messages               0.227710\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset = df.to_dict(orient='index')\n",
    "features_list = [u'poi', u'salary', u'to_messages', u'total_payments',\n",
    "       u'exercised_stock_options', u'bonus', u'restricted_stock',\n",
    "       u'shared_receipt_with_poi', u'total_stock_value', u'expenses',\n",
    "       u'from_messages', u'from_this_person_to_poi',\n",
    "       u'long_term_incentive', u'from_poi_to_this_person']\n",
    "scores_dict = dict.fromkeys(features_list[1:], np.array([]))       \n",
    "def mean_selector_score(dataset, feature_list, folds = 1000):\n",
    "    data = featureFormat(dataset, feature_list, sort_keys = True)\n",
    "    labels, features = targetFeatureSplit(data)\n",
    "    imp = Imputer(strategy='median')\n",
    "    features = imp.fit_transform(features)\n",
    "    cv = StratifiedShuffleSplit(labels, folds, random_state = 42)\n",
    "    selector = SelectKBest(k='all')\n",
    "    for train_idx, test_idx in cv: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )\n",
    "        \n",
    "        selector.fit(features_train, labels_train)\n",
    "        for score, feature in zip(selector.scores_, features_list[1:]):\n",
    "            scores_dict[feature] = np.append(scores_dict[feature], score)\n",
    "mean_selector_score(my_dataset, features_list)\n",
    "scores_df = pd.DataFrame.from_dict(scores_dict)\n",
    "scores_df.mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_list = [u'poi', u'exercised_stock_options', u'total_stock_value', u'bonus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No scaling was necessary since both attempted classifiers would not be affected by this. \n",
    "\n",
    "A feature that was engineered was the percentage of messages sent to POIs and received from POIs. The thought was maybe the overall percentage would be more predictive than a raw number since some people naturally send more e-mails than others, but if someone had larger percentages of correspondence of POIs, that might be worth looking at. The new feature 'sent_to_poi_pct' did end up being picked by SelectKBest, but resulted in almost exactly the same values in my evaluation metrics with SelectKBest(k=6). These were not included in the final classifier since I could get the same accuracy with fewer features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['sent_to_poi_pct'] = df['from_this_person_to_poi'] / df['from_messages']\n",
    "df['received_from_poi_pct'] = df['from_poi_to_this_person'] / df['to_messages']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem for classifying was the amount of null values, so an imputer was used in the final pipeline to fill NAs with the median of the column. Median was chosen as its a better fit for the wild variance seen in some of the financial data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final algorithm settled on was Gaussian Naive Bayes, GaussianNB(), in a pipeline with an Imputer filling null values with medians for that column. \n",
    "\n",
    "Note about Imputer as part of pipeline: Optimally the dataset would have medians for the entire dataset and not only the current training set. To refactor my_dataset to the proper dictionary form with median values proved very difficult. However, it was easy to alter test_classifier to fill median values before the StratifiedShuffleSplit and this achieved comparable (actually better) accuracy overall. \n",
    "\n",
    "I also tried scikit's logistic regression with built in cross validation, LogisticRegressionCV(). Both classifier algorithms performed better in precision than in recall. However, the LogisticRegressionCV algorithm resulted in significantly longer training and testing time, probably due to the cross validation it performed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric item: “tune the algorithm”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the parameters of the algorithm is to set the particular values in the initialization of the algorithm to values which optimize it for the problem at hand. This is a tricky process, and if performed incorrectly it can lead to problems like overfitting on the training set, or the exact opposite and do poorly on the training set. Since I was using GaussianNB, the only parameter on my classifier that I had to tune was the priors. I also experimented with the number of features included, adding new ones to test in order of highest score to lowest. I wrote a loop to test the accuracy of the classifier given a variety of values for priors and then looked through a resulting dataframe to figure out the best prior as well as the best set of features. I settled on priors of [.15, .85] and the aforementioned features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "features_list_score_order = [u'exercised_stock_options', u'total_stock_value', u'bonus', u'salary',\n",
    "       u'long_term_incentive', u'restricted_stock', u'total_payments',\n",
    "       u'shared_receipt_with_poi', u'from_poi_to_this_person',\n",
    "       u'from_this_person_to_poi', u'expenses', u'to_messages',\n",
    "       u'from_messages']\n",
    "testing_features_list = [u'poi']\n",
    "acc = []\n",
    "prec = []\n",
    "reca = []\n",
    "acc_all = []\n",
    "prec_all = []\n",
    "reca_all = []\n",
    "results_dict = {}\n",
    "def tuneNB():\n",
    "    for i in range(1, 20):\n",
    "        acc = []\n",
    "        prec = []\n",
    "        reca = []\n",
    "        testing_features_list = [u'poi']\n",
    "        for feature in features_list_score_order:\n",
    "            testing_features_list.append(feature)\n",
    "            pipe = Pipeline([('impute', Imputer(strategy='median')), \n",
    "                    ('classify', GaussianNB(priors=[(i/2.)*.1, (1 - (i/2.)*.1)]))])\n",
    "            total_predictions, accuracy, precision, recall, f1, f2 = \\\n",
    "                test_classifier(pipe, my_dataset, testing_features_list, folds=100)\n",
    "            acc.append(accuracy)\n",
    "            prec.append(precision)\n",
    "            reca.append(recall)\n",
    "        acc_all.append(acc)\n",
    "        prec_all.append(prec)\n",
    "        reca_all.append(reca)\n",
    "        results_dict['prec' + str(i)] = prec\n",
    "        results_dict['reca' + str(i)] = reca\n",
    "        results_dict['acc' + str(i)] = acc\n",
    "tuneNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc10</th>\n",
       "      <th>acc11</th>\n",
       "      <th>acc12</th>\n",
       "      <th>acc13</th>\n",
       "      <th>acc14</th>\n",
       "      <th>acc15</th>\n",
       "      <th>acc16</th>\n",
       "      <th>acc17</th>\n",
       "      <th>acc18</th>\n",
       "      <th>acc19</th>\n",
       "      <th>...</th>\n",
       "      <th>reca12</th>\n",
       "      <th>reca13</th>\n",
       "      <th>reca14</th>\n",
       "      <th>reca15</th>\n",
       "      <th>reca16</th>\n",
       "      <th>reca17</th>\n",
       "      <th>reca18</th>\n",
       "      <th>reca19</th>\n",
       "      <th>reca2</th>\n",
       "      <th>reca3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.855714</td>\n",
       "      <td>0.856429</td>\n",
       "      <td>0.857857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.858571</td>\n",
       "      <td>0.859286</td>\n",
       "      <td>0.862143</td>\n",
       "      <td>0.862857</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>0.866429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.230</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.848571</td>\n",
       "      <td>0.847857</td>\n",
       "      <td>0.847143</td>\n",
       "      <td>0.847857</td>\n",
       "      <td>0.848571</td>\n",
       "      <td>0.848571</td>\n",
       "      <td>0.848571</td>\n",
       "      <td>0.849286</td>\n",
       "      <td>0.852143</td>\n",
       "      <td>0.857857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.843571</td>\n",
       "      <td>0.843571</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>0.846429</td>\n",
       "      <td>0.846429</td>\n",
       "      <td>0.848571</td>\n",
       "      <td>0.849286</td>\n",
       "      <td>0.851429</td>\n",
       "      <td>0.856429</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.832143</td>\n",
       "      <td>0.832143</td>\n",
       "      <td>0.832857</td>\n",
       "      <td>0.833571</td>\n",
       "      <td>0.837857</td>\n",
       "      <td>0.840714</td>\n",
       "      <td>0.849286</td>\n",
       "      <td>0.857857</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.832143</td>\n",
       "      <td>0.833571</td>\n",
       "      <td>0.836429</td>\n",
       "      <td>0.838571</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>0.848571</td>\n",
       "      <td>0.852143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.863571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.841429</td>\n",
       "      <td>0.843571</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>0.847857</td>\n",
       "      <td>0.850714</td>\n",
       "      <td>0.850714</td>\n",
       "      <td>0.851429</td>\n",
       "      <td>0.853571</td>\n",
       "      <td>0.854286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.838571</td>\n",
       "      <td>0.838571</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.838571</td>\n",
       "      <td>0.837857</td>\n",
       "      <td>0.837857</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.270</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.822857</td>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.822857</td>\n",
       "      <td>0.823571</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.832143</td>\n",
       "      <td>0.834286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.822857</td>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.823571</td>\n",
       "      <td>0.823571</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.832143</td>\n",
       "      <td>0.833571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.822857</td>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.823571</td>\n",
       "      <td>0.825714</td>\n",
       "      <td>0.831429</td>\n",
       "      <td>0.835000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.822857</td>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.822857</td>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.824286</td>\n",
       "      <td>0.829286</td>\n",
       "      <td>0.830714</td>\n",
       "      <td>0.833571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.817143</td>\n",
       "      <td>0.817857</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.823571</td>\n",
       "      <td>0.827143</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.831429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.816429</td>\n",
       "      <td>0.817857</td>\n",
       "      <td>0.819286</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.822857</td>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.822143</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.826429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       acc10     acc11     acc12     acc13     acc14     acc15     acc16  \\\n",
       "0   0.855714  0.856429  0.857857  0.857143  0.858571  0.859286  0.862143   \n",
       "1   0.848571  0.847857  0.847143  0.847857  0.848571  0.848571  0.848571   \n",
       "2   0.843571  0.843571  0.845000  0.846429  0.846429  0.848571  0.849286   \n",
       "3   0.832143  0.832143  0.832857  0.833571  0.837857  0.840714  0.849286   \n",
       "4   0.830000  0.832143  0.833571  0.836429  0.838571  0.845000  0.848571   \n",
       "5   0.839286  0.841429  0.843571  0.845000  0.847857  0.850714  0.850714   \n",
       "6   0.838571  0.838571  0.840000  0.840000  0.839286  0.838571  0.837857   \n",
       "7   0.822143  0.822143  0.822857  0.822143  0.822143  0.822857  0.823571   \n",
       "8   0.822143  0.822143  0.822857  0.822143  0.822143  0.823571  0.823571   \n",
       "9   0.822143  0.822857  0.822143  0.822143  0.822143  0.821429  0.823571   \n",
       "10  0.822143  0.822857  0.822143  0.822857  0.822143  0.822143  0.824286   \n",
       "11  0.817143  0.817857  0.820000  0.821429  0.821429  0.821429  0.823571   \n",
       "12  0.816429  0.817857  0.819286  0.820000  0.822143  0.822857  0.822143   \n",
       "\n",
       "       acc17     acc18     acc19  ...    reca12  reca13  reca14  reca15  \\\n",
       "0   0.862857  0.865714  0.866429  ...     0.265   0.260   0.255   0.255   \n",
       "1   0.849286  0.852143  0.857857  ...     0.255   0.255   0.255   0.255   \n",
       "2   0.851429  0.856429  0.865714  ...     0.325   0.325   0.325   0.325   \n",
       "3   0.857857  0.865000  0.870000  ...     0.325   0.325   0.325   0.325   \n",
       "4   0.852143  0.857143  0.863571  ...     0.350   0.350   0.350   0.345   \n",
       "5   0.851429  0.853571  0.854286  ...     0.345   0.335   0.330   0.330   \n",
       "6   0.837857  0.840000  0.840000  ...     0.275   0.270   0.265   0.260   \n",
       "7   0.828571  0.832143  0.834286  ...     0.205   0.200   0.200   0.190   \n",
       "8   0.828571  0.832143  0.833571  ...     0.205   0.200   0.200   0.195   \n",
       "9   0.825714  0.831429  0.835000  ...     0.200   0.200   0.200   0.190   \n",
       "10  0.829286  0.830714  0.833571  ...     0.200   0.200   0.195   0.190   \n",
       "11  0.827143  0.828571  0.831429  ...     0.205   0.195   0.195   0.190   \n",
       "12  0.822143  0.825000  0.826429  ...     0.205   0.205   0.205   0.205   \n",
       "\n",
       "    reca16  reca17  reca18  reca19  reca2  reca3  \n",
       "0    0.255   0.255   0.255   0.230  1.000  0.615  \n",
       "1    0.255   0.255   0.255   0.255  0.320  0.320  \n",
       "2    0.325   0.325   0.325   0.325  0.430  0.425  \n",
       "3    0.325   0.325   0.325   0.320  0.385  0.355  \n",
       "4    0.330   0.330   0.320   0.310  0.475  0.455  \n",
       "5    0.325   0.325   0.325   0.300  0.420  0.380  \n",
       "6    0.255   0.240   0.225   0.205  0.310  0.295  \n",
       "7    0.190   0.190   0.190   0.185  0.255  0.240  \n",
       "8    0.190   0.190   0.190   0.185  0.260  0.240  \n",
       "9    0.190   0.190   0.190   0.190  0.250  0.230  \n",
       "10   0.190   0.190   0.190   0.185  0.245  0.225  \n",
       "11   0.190   0.190   0.190   0.190  0.225  0.210  \n",
       "12   0.195   0.190   0.190   0.190  0.240  0.235  \n",
       "\n",
       "[13 rows x 36 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(results_dict)\n",
    "\n",
    "drop_me = []\n",
    "for i in range(1,10):\n",
    "    if (pd.concat([(test_df['prec' + str(i)] > .38),\n",
    "       (test_df['reca' + str(i)] > .42),\n",
    "       (test_df['acc' + str(i)] > .82)], axis=1).sum(axis=1) == 3).sum() == 0:\n",
    "        drop_me.extend(['prec' + str(i), 'reca' + str(i), 'acc' + str(i)])\n",
    "test_df = test_df.drop(drop_me, axis=1)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('impute', Imputer(strategy='median')),\n",
    "        ('classify', GaussianNB(priors=[.15, .85]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric item: “validation strategy”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation is the process by which data is set aside or used so as to make sure a model performs well on data it hasn't seen yet. If this isn't handled well, we can end up overfitting the training data. This would leave us with a model that didn't generalize well or handle cases it had not yet seen. A mistake that can be made is letting too much knowledge of your test set leak into parameters used for the training set. In this case, we end up overfitting to the test set, even though it's not even part of our training data. \n",
    "\n",
    "My final method for validating my analysis was by using the provided test_classifier function, which uses Stratified Shuffle Split with a large number of folds. Some preliminary testing was also performed by a simple train/test set split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('impute', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('classify', GaussianNB(priors=[0.15, 0.85]))])\n",
      "\tAccuracy: 0.82393\tPrecision: 0.39503\tRecall: 0.43750\tF1: 0.41518\tF2: 0.42829\n",
      "\tTotal predictions: 14000\tTrue positives:  875\tFalse positives: 1340\tFalse negatives: 1125\tTrue negatives: 10660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_predictions, accuracy, precision, recall, f1, f2 = \\\n",
    "    test_classifier(pipe, my_dataset, features_list, folds=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm's overall accuracy is 82.39%. This sounds respectable, but it does score lower than the accuracy of a naive algorithm which only assigned False to every row's POI. This would end up with an accuracy of about 88% in the current dataset. However, this would also end up with a 0 in two other measures that our used algorithms scores much better in: precision and recall.\n",
    "\n",
    "Precision is the number of correctly identified POIs divided by the sum of correct POIs and falsely identified POIs. This is a measure of how often the algorithm \"got it right\" when it thought someone was a POI. So about 39.5% of the time that the algorithm thinks a person is a POI, it's actually right that they are. A respectable performance given the few features needed to get there, but it's certainly not good enough to determine whether someone should be charged with a crime or not.\n",
    "\n",
    "On the other hand, recall is the number of correctly identified POIs divided by the sum of correct POIs and POIs that were missed. This is a measure of how often the algorithm missed identifying a POI it was looking at. At about 43.75% of the time when given a POI, the algorithm realized it was in fact a POI. Again, a respectable performance, but not one good enough to automatically assume someone is innocent if the algorithm said they were."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
